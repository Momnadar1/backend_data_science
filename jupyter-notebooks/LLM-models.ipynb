{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, how are you? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello! hello! hello! hello! hello! hello!!!!!!!!! hello!!! hello! hello!!!!!!!!!!!!!!! hello!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def bert_generate_text(input_text):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "user_input = 'Hello, how are you?'\n",
    "response = bert_generate_text(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: ##hood\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "def predict_masked_word(sentence):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize input sentence and find the index of the masked token\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    \n",
    "    # Get the logits for the masked token and find the top prediction\n",
    "    mask_token_logits = output.logits[0, mask_token_index, :]\n",
    "    top_token_id = torch.argmax(mask_token_logits, dim=-1)\n",
    "    \n",
    "    # Decode the predicted token ID back to a word\n",
    "    predicted_token = tokenizer.decode(top_token_id)\n",
    "    return predicted_token\n",
    "\n",
    "sentence = \"Once upon a time, there was person [MASK]?\"\n",
    "predicted_word = predict_masked_word(sentence)\n",
    "print(f\"Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when the world was still a little bit more open to the idea of a world where people could live in peace and harmony.\n",
      "\n",
      "\"But now, we have a world where people are not afraid to be themselves. We have a world where people are not afraid to be themselves. We have a world\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    output = model.generate(input_ids, max_length=65, num_return_sequences=1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "input_text = \"There was a time\"\n",
    "generated_text = gpt2_generate_response(input_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Fine Tuning ----------------------\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokenized_text = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "            self.input_ids.append(tokenized_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_ids[idx])\n",
    "\n",
    "# Example input texts (split your data into smaller chunks if necessary)\n",
    "texts = [\n",
    "    \"Your very long input text here. \" * 100,  # Repeat to simulate a long text\n",
    "    \"Another example of a long input text.\" * 100\n",
    "]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch.squeeze(0)  # Remove batch dimension\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss - {loss.item()}\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when I was a little bit of a nerd. I was a little bit of a nerd. I was a little bit of a nerd. I was a little bit of a nerd. I was a little bit of a nerd.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "\n",
    "input_text = \"There was a time\"\n",
    "input_ids = finetuned_tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = finetuned_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(finetuned_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
